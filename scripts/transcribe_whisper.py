#!/usr/bin/env python3
"""
transcribe_whisper.py - ä½¿ç”¨ faster-whisper large-v3 è½¬å½•è®²é“éŸ³é¢‘
"""
import sys
import os
from pathlib import Path

try:
    from faster_whisper import WhisperModel
except ImportError:
    print("âŒ faster-whisper not installed: pip3 install faster-whisper")
    sys.exit(1)

BASE_DIR = Path(__file__).parent.parent
DOWNLOADS_DIR = BASE_DIR / "downloads"

# 6 ä¸ªéœ€è¦è½¬å½•çš„è®²é“
SERMONS = [
    "Admitting",
    "The_Glory_of_the_Incarnation",
    "Radical_Generosity",
    "Seeking_the_Kingdom",
    "Thy_Will_Be_Done",
    "The_Power_of_the_Incarnation",
]


def transcribe_sermon(model, sermon_name: str) -> str:
    """Transcribe a single sermon, return full text."""
    mp3_path = DOWNLOADS_DIR / f"{sermon_name}.mp3"
    if not mp3_path.exists():
        print(f"  âŒ {mp3_path} not found")
        return ""

    print(f"  ğŸ™ï¸  Transcribing {sermon_name}...")
    segments, info = model.transcribe(
        str(mp3_path),
        language="en",
        beam_size=5,
        vad_filter=True,  # Filter out silence
    )

    text_parts = []
    for segment in segments:
        text_parts.append(segment.text.strip())

    full_text = " ".join(text_parts)
    print(f"  âœ… {sermon_name}: {len(full_text):,} chars")
    return full_text


def main():
    print("ğŸ“¥ Loading faster-whisper large-v3 model...")
    print("   (First run will download ~3GB model)")
    model = WhisperModel("large-v3", device="cpu", compute_type="int8")
    print("âœ… Model loaded\n")

    for sermon_name in SERMONS:
        text = transcribe_sermon(model, sermon_name)
        if not text:
            continue

        # Write to output txt file
        out_path = DOWNLOADS_DIR / f"{sermon_name}.txt"
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(text)
        print(f"  ğŸ“ Saved: {out_path}\n")

    print("ğŸ‰ All done!")


if __name__ == "__main__":
    main()
